---
title: "Knowledge-enhanced biomarker discovery"
subtitle: "Trifels Spring School 2025: AI in Bioinformatics"
author: David Selby
institute: DFKI
date: 2025-03-24
date-format: Do MMMM YYYY
from: markdown+emoji
title-slide-attributes:
    data-background-image: images/burg_trifels2.jpg
    data-background-size: contain
    data-background-opacity: "0.5"
    data-background-color: black
format:
    revealjs:
        slide-number: true #c/t
        logo: images/dfki_black.png
        theme: default
        navigation-mode: linear # linear or vertical
        chalkboard: true # drawing with 'c' and 'b' hotkeys
        toc: true
        toc-depth: 1
        toc-title: Agenda
        number-sections: true
        number-depth: 1 # subsection numbering
        transition: fade
        background-transition: fade
        embed-resources: false # true is slower but self-contained
        mermaid:
            theme: forest
        hash-type: number
engine: knitr
---

# Preamble {.unnumbered .unlisted background-image="images/annweiler.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

```{css, label='css', echo=FALSE}
/* Style the table of contents */
#TOC ul li {
    list-style-type: none;
}
#TOC a {
    font-size: larger;
    color: black;
    font-weight: bold;
}
```

## David Selby {.unnumbered}

::::: columns
::: column
![](images/david.jpg)

**Senior Researcher** {{< iconify twemoji:flag-germany title="Germany" >}} {{< iconify twemoji:flag-united-kingdom title="United Kingdom" >}}\
Data Science & its Applications\
[david.selby\@dfki.de](mailto:david.selby@dfki.de)
:::

::: column
![](images/dfki_logo_full.png) ![](images/rptu_logo.png)
:::
:::::

##  {.unnumbered background-image="images/annweiler.jpg" background-size="cover" background-color="black"}

::: footer
© David Selby
:::

##  {.unnumbered background-image="images/burg_trifels1.jpg" background-size="cover" background-color="black"}

::: footer
© David Selby
:::

##  {.unnumbered background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black"}

::: footer
© David Selby
:::

##  {.unnumbered background-image="images/burg_aussicht.jpg" background-size="cover" background-color="black"}

::: footer
© David Selby
:::

## Workshop objectives {{< iconify pajamas:issue-type-objective >}} {.unnumbered}

By the end of this session, we aim to:

-   {{<iconify twemoji:brain >}} **Understand** role of prior knowledge in biomarker discovery
-   {{<iconify twemoji:dna >}} **Learn** how to integrate biological context into workflows
-   {{<iconify twemoji:hammer-and-wrench >}} **Explore** tools for knowledge-guided analysis
-   {{<iconify twemoji:construction >}} **Discuss** challenges in knowledge-guided AI for biomedicine

# Introduction & Motivation {.smaller background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

##  {.center .unnumbered}

[**Knowledge enhanced**]{style="color: rgb(029, 058, 143);"} [(multi-omics)]{style="color: rgb(236, 097, 159);"} [**biomarker discovery**]{style="color: rgb(106, 191, 163);"}

. . .

:::::: {.columns layout="[0.3, 0.3, 0.3]"}
::: column
{{<iconify twemoji:brain >}}

Why? Where? How?
:::

::: column
{{<iconify twemoji:abacus >}}

What makes it hard?
:::

::: column
{{<iconify twemoji:dna >}}

What are we looking for?
:::
::::::

------------------------------------------------------------------------

![](https://curatime.org/assets/610bc0d76b2b3a8ddbeeb5df_curatime_logo.svg){width="40%"}

![](https://curatime.org/img/containers/assets/curatime_unsere-losung_transparent.png/79c9599f71a662d02213fc803b87d53f.webp)

------------------------------------------------------------------------

```{mermaid}
%%| label: biomarker-history

timeline
    title A short history of biomarker discovery

%%    section Before AI Integration
      1949 : Introduction of 'biochemical markers' (Mundkur)
      1970s : Term 'biomarker' coined (1973) : Prostate-Specific Antigen discovered (1979)
      1990s : BRCA1 gene discovered by Mary-Claire King (1990) : BRCA1 gene cloned (1994)
%%    section After AI Integration
      2000s : 'biomarker' formally defined* (2000) : NGS introduced (2005) : QRISK (2007)
      2010s : liquid biopsy : endotypes : CRISPR : ImageNet : GANs : BERT : SHAP
      2020s : AlphaFold : GPT-3 : Blood-based biomarkers for Alzheimer's
```

::: aside
\***Definition**: "indicators of biologic/pathogenic processes/responses ... frequently measured and evaluated"
:::

::: footer
Bodaghi, Fattahi & Ramazani. Biomarkers. *Heliyon* (2023). doi:[10.1016/j.heliyon.2023.e13323](https://doi.org/10.1016/j.heliyon.2023.e13323)

Ng, S., Masarone, S., Watson, D. et al. The benefits and pitfalls of machine learning for biomarker discovery. *Cell Tissue Res* (2023). doi:[10.1007/s00441-023-03816-z](https://doi.org/10.1007/s00441-023-03816-z)
:::

------------------------------------------------------------------------

### Key challenges

-   High dimensionality & small sample sizes ($p \gg n$)
-   Heterogeneous data modalities
-   Complexity–interpretability tradeoff
-   Validation in diverse cohorts
-   FAIRness of data, methods and tools

::: callout-note
### Signatures or biomarkers

(Sets of) features predictive of a biological outcome
:::

::: notes
-   Dimensionality reduction
-   Integrating different molecular data, as well as non-molecular clinical data and imaging
-   Explainability, interpretability
-   Heavy reliance on TCGA and common tools may not generalize
-   FAIR = Findable, Accessible, Interoperable, Reusable
:::

## Multi-omics biomarker discovery

::::: columns
::: column
-   Contrasts with **single-omics**
-   Combines
    -   genomics, transcriptomics, proteomics, metabolomics, ...
    -   cliniomics, radiomics, ...
-   Identify robust **signatures** for disease diagnosis, prognosis or treatment
:::

::: column
![](images/multiomics.png)
:::
:::::

## Knowledge-intensive machine learning

-   **Prior knowledge** can guide model training
-   **Interpretability** is crucial for clinical adoption
-   Easy to **overfit** in high-dimensional space
-   Biomedical analytical insights not easy to **reproduce**
    -   data wrangling
    -   domain expertise
    -   model interpretations

# Prior knowledge {.smaller background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

## What is prior knowledge?

-   scientific publications in literature
-   open datasets (e.g. TCGA, OpenML, UCI)
-   domain-specific databases (e.g. KEGG, Reactome, GO)
-   networks data (e.g. protein-protein interactions)
-   ontologies
-   expert knowledge (Bayesian decision-making)

. . .

How can prior knowledge be encoded in a **transparent, reproducible** way?

## Approaches

1.  Regularization
2.  Knowledge graphs
3.  Biologically-informed architectures

## Information theory {.smaller}

**Entropy** $H(X)$ quantifies uncertainty in a random variable $X$

-   Higher entropy → more uncertainty
-   Lower entropy → more certainty

**Prior knowledge** reduces entropy:

-   Narrows the hypothesis space
-   Focuses on biologically plausible solutions

::: callout-important
## Example: biomarker discovery

**Without** prior knowledge, search space includes **all possible gene combinations** (high entropy, expensive).

**With** prior knowledge, focus on **pathways, interactions, or known gene sets** (low entropy, faster convergence).
:::

##  {.unnumbered .smaller}

**Mutual information** $I(X;Y)$ measures shared information between $X$ (data) and $Y$ (prior knowledge).

Higher $I(X;Y)$ → more effective integration of prior knowledge

Mutual information can be expressed $$
I(X;K) = H(X) - H(X|K),
$$ where

-   $H(X)$ is the entropy (uncertainty) of the data $X$
-   $H(X|K)$ is the conditional entropy of $X$ given prior knowledge $K$.

Reduction in entropy is equivalent to the mutual information: $\Delta H = I(X;K).$

::: callout-tip
The greater the mutual information $I(X;K)$, the more effective the prior knowledge $K$ is in narrowing the hypothesis space.
:::

::: footer
Shannon, C.E. A Mathematical Theory of Communication. *Bell System Technical Journal* (1948). doi:[10.1002/j.1538-7305.1948.tb01338.x](https://doi.org/10.1002/j.1538-7305.1948.tb01338.x)
:::

------------------------------------------------------------------------

![Entropy $H(\cdot)$, conditional entropy $H(\cdot\mid\cdot)$ and mutual information $I(\cdot; \cdot)$](https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg)

------------------------------------------------------------------------

::: callout-note
### Example of entropy in biomarker discovery

**Without Prior Knowledge**:

-   Searching among **10,000 genes**.
-   Entropy: $H(X) = \log_2(10,000) \approx 13.29 \text{ bits.}$

**With Prior Knowledge**:

-   Prior knowledge narrows the search to **100 candidate genes**.
-   Entropy: $H(X|K) = \log_2(100) \approx 6.64 \text{ bits.}$

**Mutual Information**

-   Reduction in entropy:\
    $I(X;K) = H(X) - H(X|K) = 13.29 - 6.64 = 6.65 \text{ bits.}$
:::

##  {.unnumbered .center}

::: callout-warning
### Lower entropy ≠ correctness

Prior knowledge (and data) can still be **wrong/biassed**!

Then *smaller* hypothesis space → barking up the *wrong* tree.
:::

# Multi-omics integration {.smaller background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

##  {.unnumbered background-image="https://www.researchgate.net/publication/346929297/figure/fig1/AS:11431281241521883@1715131118932/The-complexity-of-multi-omics-merger-of-omics-driven-biology-data-science-informatics.tif" background-size="contain"}

::: footer
Krassowski at al. (2020). State of the Field in Multi-Omics Research. *Frontiers in Genetics.* doi:[10.3389/fgene.2020.610798](https://doi.org/10.3389/fgene.2020.610798)
:::

## Single-omics methods

| Task | Tools |
|-------------------------------|-----------------------------------------|
| {{< iconify twemoji:bar-chart >}} Differential expression analysis | DESeq2, edgeR, limma |
| {{< iconify vaadin:cluster >}} Clustering samples/features | $k$-means, hierarchical clustering, $t$-SNE, UMAP |
| {{< iconify twemoji:dna >}} Pathway enrichment analysis | GSEA, Reactome |
| {{< iconify twemoji:chart-increasing >}} Predictive modelling | GLMs, random forests, SVM, XGBoost, NNs |

## Single-omics challenges {.smaller}

| Challenge     | Methods                  | Tools/Packages                      |
|-------------------|-----------------------------|-------------------------|
| $p \gg n$     | Dimensionality reduction | PCA, t-SNE, UMAP, LASSO, ElasticNet |
| Batch effects | Batch effect correction  | ComBat (sva), limma, Harmony        |
| Missing data  | Imputation               | MICE, KNN imputation, MissForest    |

. . .

::: callout-tip
Single-omics data is **tabular**, for which **tree-based models** (e.g. random forests) can outperform deep learning.
:::

::: footer
Grinszjtajn et al. [Why do tree-based models still outperform deep learning on typical tabular data?](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets_and_Benchmarks.html) *NeurIPS.* (2022)
:::

------------------------------------------------------------------------

::: callout-warning
### Multi-omics data bring extra challenges

-   **Integration** of different data types
-   **Interpretation** of complex interactions
:::

## Multi-omics datasets {.center}

::::: columns
::: column
![](images/multiomics.png)
:::

::: column
-   Tabular data
-   High-dimensional
-   Small samples
-   **Multimodal structure**
:::
:::::

## Why deep learning?

Why not just use classical methods?

### Classical approaches

Multi-omics factor analysis (MOFA)

:   Unsupervised, generalization of PCA

Canonical correlation analysis (CCA)

:   Find linear combinations of features that are maximally correlated

Grouped LASSO

:   (Linearly) penalize groups of features together

## Why deep learning?

Why not just use classical methods? Isn't it tabular data?

::: incremental
-   **Non-linear** relationships
-   **Interactions** between features
-   **Representation learning** from raw data
-   **End-to-end** learning
:::

. . .

```{mermaid}
%%| label: classical-vs-dl-pipeline

flowchart LR
    A[Feature Selection] --> B[Modelling]
    B --> C[Pathway Enrichment]
```

::: footer
Park et al. Sparse overlapping group lasso for integrative multi-omics analysis. *J Comput Biol.* (2015) doi:[10.1089/cmb.2014.0197](https://doi.org/10.1089/cmb.2014.0197)
:::

## Deep learning architectures

::: callout-note
### Question

Which neural network architectures are suitable for omics?
:::

## Multimodal fusion {.smaller}

When should we combine omics layers?

![](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f809/8829812/7f899d7be8ff/gr2.jpg)

::: footer
Cai, Poulos, Liu & Zhong. Machine learning for multi-omics data integration in cancer. *iScience.* (2022). doi:[10.1016/j.isci.2022.103798](https://doi.org/10.1016/j.isci.2022.103798)
:::

## Multimodal fusion

When should we combine omics layers?

Early

:   easier, loss of information, worse performance\*

Intermediate (mixed, joint)

:   modality-specific layers, but harder to train

Late

:   may not capture interactions

::: footer
\*Hauptmann, T., Kramer, S. A fair experimental comparison of neural network architectures for latent representations of multi-omics for drug response prediction. *BMC Bioinformatics* 24, 45 (2023). doi:[10.1186/s12859-023-05166-7](https://doi.org/10.1186/s12859-023-05166-7)
:::

## Late fusion (MOLI)

![](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3f5e/6612815/2f27faef728d/btz318f1.jpg)

::: footer
Sharifi-Noghabi et al. *Bioinformatics*. 2019. doi: [10.1093/bioinformatics/btz318](https://doi.org/10.1093/bioinformatics/btz318)
:::

::: notes
Hello, world
:::

## Graph neural networks (GNNs)

![MOGAT](https://www.mdpi.com/ijms/ijms-25-02788/article_deploy/html/images/ijms-25-02788-g001.png)

::: footer
Tanvir et al. MOGAT. *International Journal of Molecular Sciences*, (2024). doi:[10.3390/ijms25052788](https://doi.org/10.3390/ijms25052788)
:::

## Model explanations

![Can we call hidden nodes in neural networks 'biomarkers'?](images/nn.png){fig-align="center"}

## Model explanations

Input-level explanations:

-   $p$-values, features importance
-   DeepLIFT
-   SHAP
-   LIME

→ *post-hoc* gene-set enrichment analysis (GSEA) or "pathway analysis"

![](https://reactome.org/templates/favourite/images/logo/logo.png)   <!--![](https://upload.wikimedia.org/wikipedia/en/8/80/KEGG_database_logo.gif)-->

## Gene set enrichment analysis {.smaller}

1.  Set of genes $G = \{g_1, g_2, \dots, g_N\}$. Order by ranking metric $S(g_i)$ (e.g. *t*-statistic)

2.  Compute **enrichment score** using running sum statistics, or **overrepresentation score** with hypergeometric test: $$
    P(X = x) = \frac{\binom{M}{x} \binom{N-M}{n-x}}{\binom{N}{n}}
    $$ with $p$-value $$
    p = \sum_{i=x}^{\min(M, K)} P(X = i).
    $$

## What is a pathway (gene set)?

![Reactome pathways](https://reactome.org/images/baac009f1.jpeg)

A pathway is a set of genes that are known to interact in a biological process.

::: footer
<https://reactome.org/>
:::

## Pathway databases

::::: columns
::: column
![](https://reactome.org/templates/favourite/images/logo/logo.png)
:::

::: column
![](https://upload.wikimedia.org/wikipedia/en/8/80/KEGG_database_logo.gif)
:::
:::::

![](https://geneontology.org/assets/go-logo.png)

##  {.centered}

::: callout-info
### Pathway enrichment analysis

What are the disadvantages of using *post-hoc* GSEA or GSOA?
:::

# Visible neural networks {.smaller background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

## Feedforward neural network

![{{<iconify twemoji:magnifying-glass-tilted-left >}} Where are the biomarkers?](images/nn.png){fig-align="center"}

## Visible neural network (VNN)

![DCell](https://ars.els-cdn.com/content/image/1-s2.0-S0092867418307190-gr2_lrg.jpg)

::: footer
Yu et al. Visible Machine Learning for Biomedicine. *Cell* (2018) doi:[10.1016/j.cell.2018.05.056](https://doi.org/10.1016/j.cell.2018.05.056)
:::

------------------------------------------------------------------------

## Knowledge-primed neural network

![](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs13059-020-02100-5/MediaObjects/13059_2020_2100_Fig1_HTML.png)

::: footer
Fortelny & Bock. Knowledge-primed neural networks. *Genome Biology* (2020). doi:[10.1186/s13059-020-02100-5](https://doi.org/10.1186/s13059-020-02100-5)
:::

------------------------------------------------------------------------

## Biologically informed neural network (BINN)

![P-Net](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-021-03922-4/MediaObjects/41586_2021_3922_Fig1_HTML.png)

::: footer
Elmarakeby et al. Biologically informed deep neural network for prostate cancer discovery. *Nature*, (2021). doi:[10.1038/s41586-021-03922-4](https://doi.org/10.1038/s41586-021-03922-4)
:::

## VAE enhanced by gene annotations (VEGA)

![VEGA](https://vega-documentation.readthedocs.io/en/latest/_images/vega.png)

::: footer
Seninge, et al. VEGA. *Nature Communications* (2021). doi:[10.1038/s41467-021-26017-0)](https://doi.org/10.1038/s41467-021-26017-0)
:::

## Anatomy of a BINN/VNN

![](images/binn_natrevgen.png){width="80%"}

::: footer
Selby, D.A. et al. *Nature Reviews Genetics* (2025). doi:[10.1038/s41576-025-00826-1](https://doi.org/10.1038/s41576-025-00826-1)
:::

## How to build a VNN

1.  Start with dense sequential neural network
2.  Use adjacency matrix of inputs → pathways as a **masking matrix**
3.  (Optional) for next layer, mask = mapping of pathways → higher pathways
4.  Omit all masked weights from backpropagation

::: callout-warning
What if the pathways aren't all the same depth?
:::

## Knowledge graph approach

![GLUE architecture](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41587-022-01284-4/MediaObjects/41587_2022_1284_Fig1_HTML.png)

::: footer
Cao & Gao. Multi-omics single-cell data integration and regulatory inference with graph-linked embedding. *Nature Biotech.* (2022). doi:[10.1038/s41587-022-01284-4](https://doi.org/10.1038/s41587-022-01284-4)
:::

::: notes
Is this interpretable?
:::

## Pathway discovery with BINNs

![PathExpSurv](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12859-023-05535-2/MediaObjects/12859_2023_5535_Fig1_HTML.png)

::: footer
Hou et al. PathExpSurv. *BMC Bioinformatics*, (2023). doi:[10.1186/s12859-023-05535-2](https://doi.org/10.1186/s12859-023-05535-2)
:::

## Interpretable AutoML discovery

::::: columns
::: {.column .callout-tip}
### Open question

If (1) VNNs perform better, and (2) Pathway DBs are incomplete; then

**Can neural architecture search over VNNs discover new pathways?**
:::

::: column
![](https://towardsdatascience.com/wp-content/uploads/2020/09/1nicFUkeUpWMW1w_hUVtZiw.png)
:::
:::::

::: footer
Han et al. (2015). arXiv:[1506.02626v3](https://arxiv.org/abs/1506.02626v3)
:::

# Hands-on {.smaller background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

## Accessible tools for BINNs {.smaller}

...would be a very good idea

. . .

::::: {.columns layout="[0.5, -0.05, 0.5]"}
::: column
### [BINN](https://github.com/InfectionMedicineProteomics/BINN)

-   `pip install binn`
-   supervised learning
-   proteomics or single-omics
-   omics data: as `pandas` or csv
-   pathways: 'Reactome' or own csv (`parent,child`)
:::

::: column
### [VEGA](https://github.com/LucasESBS/vega)

-   `pip install scvega`
-   unsupervised learning
-   transcriptomics
-   omics data: as Anndata
-   pathways: GMT from [MSigDB](https://www.gsea-msigdb.org/gsea/index.jsp)
:::
:::::

. . .

...and various other paper repos you can `git clone`.

. . .

Nothing for R... [yet](https://torch.mlverse.org/)!

##  {.center}

::: callout-warning
### What's missing?

-   multi-omics
-   integration with clinical data
-   survival analysis (censored time-to-event)
-   R + BioConductor
:::

## The `binn` package

::: callout-tip
### Installation

``` bash
pip install binn==0.1.1         # 0.1.0 has a bug
```
:::

::: callout-tip
### Usage

``` python
from binn import BINN
binn = BINN(
    data_matrix=input_data,
    network_source="reactome",
    input_source="uniprot",
    n_layers=4,
    dropout=0.2
)
```
:::

::: callout-note
### Documentation

<https://infectionmedicineproteomics.github.io/BINN/>
:::

## The binn package - inputs {.smaller}

What's required:

:::::: {.columns layout="[0.3, -0.05, 0.3, -0.05, 0.3]"}
::: column
**`data_matrix`**

-   $n \times p$ matrix of omics data

**`mapping`**

-   $p \times 2$ 2 column matrix of features to pathways
-   or `'uniprot'`
:::

::: column
**`pathways`**

-   $? \times 2$ matrix: parent → child pathway
-   or `'reactome'`
:::

::: column
**`design_matrix`**

-   matrix $n \times 2$ of targets `sample,group`
-   `group` = output class
:::
::::::

::: footer
<https://infectionmedicineproteomics.github.io/BINN/>
:::

## The `binn` package - visualization

![Node importance plot](https://infectionmedicineproteomics.github.io/BINN/img/interpreted_binn.png)

::: footer
<https://infectionmedicineproteomics.github.io/BINN/>
:::

## Worksheet

![](images/qr_code.png){fig-align="center"}

Colab notebook and slides: [github.com/datasciapps/trifels2025](https://github.com/datasciapps/trifels2025)

# Discussion {.smaller background-image="images/burg_trifels2.jpg" background-size="cover" background-color="black" background-opacity="0.5"}

## Challenges {.smaller}

-   **Lack of Robust Tools**: Few standardized frameworks, poor interpretability and limited benchmarks.
-   **Alternative Models**:
    -   **Classical methods**: faster, more interpretable, statistically robust
    -   **GNNs**: Better structural representation but need high-quality pathway graphs.\
    -   **Transformers/LLMs**: Strong sequence modeling but lack explicit pathway structure.\
    -   **Hybrid Approaches**: Combining VNNs with GNNs or transformers may improve learning.\
-   **Pathway Databases Are Incomplete**: KEGG, Reactome, and GO suffer from curation bias, lack context specificity, and may be outdated.

## Open questions {.smaller}

### Biological fidelity

-   Is a BINN really a digital twin of a cell?
-   Biological sparsity or just noisy sparsity?

### Model validation

-   Synthetic and experimental data for validation
-   Uncertainty-aware architectures to quantify confidence in biological predictions.

### Performance vs interpretability

-   Mechanistic insight vs. predictive accuracy: which should take priority?
-   Do black-box models merely reinforce existing biases in pathway databases?

## Things we didn't cover today

-   Causal inference
-   Dynamic updating of knowledge graphs
-   Bayesian prior elicitation
-   GenAI: LLM agents & retrieval-augmented generation

# Thank you! {.unlisted .unnumbered}

::::: columns
::: {.column width="70%"}
 

{{<iconify twemoji:link >}} [github.com/datasciapps/trifels2025](https://github.com/datasciapps/trifels2025)\
{{<iconify twemoji:envelope >}} [david.selby\@dfki.de](mailto:david.selby@dfki.de?Subject=Trifels%20Spring%20School)
:::

::: {.column width="30%"}
![](images/qr_code.png)
:::
:::::

::: callout-note
## Further reading

Selby, D.A. et al. Beyond the black box with biologically informed neural networks. *Nature Reviews Genetics* (2025). doi:[10.1038/s41576-025-00826-1](https://doi.org/10.1038/s41576-025-00826-1)
:::

::: footer
To export these slides, hit `E` and then *Print to PDF*.
:::